{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation.py code\n",
    "import torch\n",
    "import torchaudio\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "def add_gaussian_noise(audio: torch.Tensor, noise_level: float = 0.005) -> torch.Tensor:\n",
    "    noise = torch.randn_like(audio) * noise_level\n",
    "    return audio + noise\n",
    "\n",
    "def time_stretch(audio: torch.Tensor, rate: float = 1.0) -> torch.Tensor:\n",
    "    return torchaudio.transforms.TimeStretch(n_freq=201)(audio, rate)\n",
    "\n",
    "def pitch_shift(audio: torch.Tensor, n_steps: int = 0) -> torch.Tensor:\n",
    "    return torchaudio.transforms.PitchShift(sample_rate=16000, n_steps=n_steps)(audio)\n",
    "\n",
    "def random_augment(audio: torch.Tensor) -> torch.Tensor:\n",
    "    augmentations = [\n",
    "        (add_gaussian_noise, {\"noise_level\": random.uniform(0.001, 0.01)}),\n",
    "        (time_stretch, {\"rate\": random.uniform(0.9, 1.1)}),\n",
    "        (pitch_shift, {\"n_steps\": random.randint(-2, 2)})\n",
    "    ]\n",
    "    \n",
    "    # Randomly choose 1-3 augmentations\n",
    "    num_augmentations = random.randint(1, 3)\n",
    "    chosen_augmentations = random.sample(augmentations, num_augmentations)\n",
    "    \n",
    "    for aug_func, params in chosen_augmentations:\n",
    "        audio = aug_func(audio, **params)\n",
    "    \n",
    "    return audio\n",
    "\n",
    "# Function to visualize the augmentations\n",
    "def plot_waveform(waveform, title=\"Waveform\"):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(waveform.t().numpy())\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_spectrogram(waveform, title=\"Spectrogram\"):\n",
    "    spectrogram = librosa.stft(waveform.numpy().squeeze())\n",
    "    spectrogram_db = librosa.amplitude_to_db(abs(spectrogram))\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.specshow(spectrogram_db, sr=16000, x_axis='time', y_axis='hz')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Load a sample audio file (you'll need to provide your own audio file)\n",
    "# waveform, sample_rate = torchaudio.load('path_to_your_audio_file.wav')\n",
    "\n",
    "# # Original audio\n",
    "# plot_waveform(waveform, \"Original Waveform\")\n",
    "# plot_spectrogram(waveform, \"Original Spectrogram\")\n",
    "\n",
    "# # Augmented audio\n",
    "# augmented_waveform = random_augment(waveform)\n",
    "# plot_waveform(augmented_waveform, \"Augmented Waveform\")\n",
    "# plot_spectrogram(augmented_waveform, \"Augmented Spectrogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated audio dataset methods\n",
    "def __getitem__(self, index: int) -> tuple[torch.Tensor, int]:\n",
    "    \"Returns one sample of data, and its class index\"\n",
    "\n",
    "    audio_tensor = self.load_audio(index)\n",
    "    class_name = self.paths[index].parent.name\n",
    "    class_idx = self.class_to_idx[class_name]\n",
    "    \n",
    "    # Standardize audio length and frequency\n",
    "    audio_tensor = self.standardize_audio(audio_tensor)\n",
    "    \n",
    "    # Using the AST model's correct size for transformation\n",
    "    inputs = self.feature_extractor(audio_tensor.squeeze().numpy(), return_tensors=\"pt\", sampling_rate=16000)\n",
    "    inputs = inputs['input_values'][0]  # Extract input tensor\n",
    "\n",
    "    return inputs, class_idx\n",
    "\n",
    "def standardize_audio(self, audio_tensor: torch.Tensor, target_length: int = 80000, target_sr: int = 16000) -> torch.Tensor:\n",
    "    \"\"\"Standardize audio to 5 seconds at 16kHz\"\"\"\n",
    "    current_sr = 16000  # Assuming original sample rate is 16kHz\n",
    "    \n",
    "    # Resample if necessary\n",
    "    if current_sr != target_sr:\n",
    "        audio_tensor = torchaudio.transforms.Resample(current_sr, target_sr)(audio_tensor)\n",
    "    \n",
    "    # Pad or trim to target length (5 seconds * 16000 Hz = 80000 samples)\n",
    "    if audio_tensor.size(1) < target_length:\n",
    "        audio_tensor = torch.nn.functional.pad(audio_tensor, (0, target_length - audio_tensor.size(1)))\n",
    "    else:\n",
    "        audio_tensor = audio_tensor[:, :target_length]\n",
    "    \n",
    "    return audio_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UAV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
