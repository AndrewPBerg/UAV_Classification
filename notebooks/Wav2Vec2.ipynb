{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sidewinders\\anaconda3\\envs\\Research\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "PeftModel                                                         --\n",
       "├─LoraModel: 1-1                                                  --\n",
       "│    └─Wav2Vec2ForCTC: 2-1                                        --\n",
       "│    │    └─Wav2Vec2Model: 3-1                                    (315,428,992)\n",
       "│    │    └─Dropout: 3-2                                          --\n",
       "│    │    └─Linear: 3-3                                           41,248\n",
       "==========================================================================================\n",
       "Total params: 315,470,240\n",
       "Trainable params: 8,448\n",
       "Non-trainable params: 315,461,792\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from torchinfo import summary\n",
    "\n",
    "# Load pre-trained Wav2Vec2 model\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the LoRA matrices\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    target_modules=[\"lm_head\"],  # Only apply LoRA to the classifier (lm_head)\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    ")\n",
    "\n",
    "# Apply PEFT (LoRA) to the model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Check the new model architecture with PEFT applied\n",
    "summary(peft_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DESCRIPTION\n",
    "Experiment with MIT's AST (Audio Spectrogram Transformer) for UAV Classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from AST_helper.util import AudioDataset, train_test_split_custom\n",
    "from AST_helper.engine import train, inference_loop\n",
    "from AST_helper.model import auto_extractor, custom_AST\n",
    "from AST_helper.util import save_model # noqa: F401\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "import wandb\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "display(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/Sidewinders/Research_notebooks/Drone_classification/Research/UAV_Dataset_9\"\n",
    "model_name = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "BATCH_SIZE = 4\n",
    "SEED = 42\n",
    "EPOCHS = 2\n",
    "NUM_CUDA_WORKERS = 0\n",
    "PINNED_MEMORY = True\n",
    "SHUFFLED = True\n",
    "ACCUMULATION_STEPS = 2 # multiplies by batch size for large batch size effect.\n",
    "OPTIM_LR = 0.0001\n",
    "TRAIN_PATIENCE = 5\n",
    "multiple_runs = False\n",
    "wandb_init = False\n",
    "SAVE_MODEL = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "config = {\n",
    "        \"learning_rate\": OPTIM_LR,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"num_epochs\": EPOCHS,\n",
    "        \"random_seed\" : SEED,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"loss_function\": \"CrossEntropyLoss\"\n",
    "    }\n",
    "wandb_params = {\n",
    "        \"project\": \"vanilla_AST\",\n",
    "        \"name\": \"classifier_grad_true_lowerLR\",\n",
    "        \"reinit\": False,\n",
    "        \"notes\" : \"8457 trainable params\",\n",
    "        \"tags\": [\"AST\"],\n",
    "        \"config\": config\n",
    "    }\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                                                          Param #              Trainable\n",
       "========================================================================================================================\n",
       "PeftModel (PeftModel)                                                            --                   Partial\n",
       "├─LoraModel (base_model)                                                         --                   Partial\n",
       "│    └─Wav2Vec2ForCTC (model)                                                    --                   Partial\n",
       "│    │    └─Wav2Vec2Model (wav2vec2)                                             (315,428,992)        False\n",
       "│    │    └─Dropout (dropout)                                                    --                   --\n",
       "│    │    └─Linear (lm_head)                                                     41,248               Partial\n",
       "========================================================================================================================\n",
       "Total params: 315,470,240\n",
       "Trainable params: 8,448\n",
       "Non-trainable params: 315,461,792\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature_extractor = auto_extractor(model_name)\n",
    "\n",
    "dataset_0 = AudioDataset(data_path, processor)\n",
    "shape = dataset_0[0][0].shape\n",
    "\n",
    "train_subset, test_subset, inference_subset = train_test_split_custom(dataset_0, test_size=0.2, inference_size=0.1) \n",
    "num_classes = len(dataset_0.get_classes()) \n",
    "\n",
    "model = peft_model.to(device)\n",
    "# model = custom_AST(model_name, num_classes, device)\n",
    "\n",
    "summary(model,\n",
    "        col_names=[\"num_params\",\"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_custom = DataLoader(dataset=train_subset, \n",
    "                                     batch_size=BATCH_SIZE,\n",
    "                                     num_workers=NUM_CUDA_WORKERS,\n",
    "                                     pin_memory=PINNED_MEMORY,\n",
    "                                     shuffle=SHUFFLED)\n",
    "\n",
    "test_dataloader_custom = DataLoader(dataset=test_subset,\n",
    "                                    batch_size=BATCH_SIZE, \n",
    "                                    num_workers=NUM_CUDA_WORKERS,\n",
    "                                    pin_memory=PINNED_MEMORY,\n",
    "                                    shuffle=SHUFFLED)\n",
    "\n",
    "if inference_subset: # may not be defined\n",
    "    inference_dataloader_custom = DataLoader(dataset=inference_subset,\n",
    "                                    batch_size=BATCH_SIZE, \n",
    "                                    num_workers=NUM_CUDA_WORKERS,\n",
    "                                    pin_memory=PINNED_MEMORY,\n",
    "                                    shuffle=SHUFFLED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=OPTIM_LR)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3) #TODO experiment w/ diff hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb_init:\n",
    "    wandb.init(\n",
    "            project=wandb_params.get(\"project\"),\n",
    "            config=wandb_params.get(\"config\"),\n",
    "            name=wandb_params.get(\"name\"),\n",
    "            reinit=wandb_params.get(\"reinit\", True),\n",
    "            tags=wandb_params.get(\"tags\", []),\n",
    "            notes=wandb_params.get(\"notes\", \"\"),\n",
    "            dir=wandb_params.get(\"dir\", None)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521e6656eb4c41b0a560a507d2444480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sidewinders\\anaconda3\\envs\\Research\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:862: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "results = train(model,\n",
    "                train_dataloader=train_dataloader_custom,\n",
    "                test_dataloader=test_dataloader_custom,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                loss_fn=loss_fn,\n",
    "                epochs=EPOCHS,\n",
    "                device=device,\n",
    "                accumulation_steps=ACCUMULATION_STEPS,\n",
    "                patience=TRAIN_PATIENCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sidewinders\\Research_notebooks\\Drone_classification\\Research\\notebooks\\AST_helper\\engine.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Loss: 0.4719, Accuracy: 100.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d37a5e06b54ba78651c687482b6a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>inference_accuracy</td><td>▁</td></tr><tr><td>inference_loss</td><td>▁</td></tr><tr><td>test_acc</td><td>▁▂▄▅▇▇▇▇▇▇██████████</td></tr><tr><td>test_f1</td><td>▁▂▄▅▇▇▇▇▇███████████</td></tr><tr><td>test_loss</td><td>█▇▆▅▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>test_precision</td><td>▁▃▄▆▇▇▇▇▇▇██████████</td></tr><tr><td>test_recall</td><td>▁▂▄▅▇▇▇▇▇▇██████████</td></tr><tr><td>train_acc</td><td>▁▂▄▅▆▇▇▇▇███████████</td></tr><tr><td>train_f1</td><td>▁▂▄▆▆▇▇▇▇███████████</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_precision</td><td>▁▂▅▆▇▇▇▇▇███████████</td></tr><tr><td>train_recall</td><td>▁▂▄▆▆▇▇▇▇███████████</td></tr><tr><td>train_time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>inference_accuracy</td><td>1</td></tr><tr><td>inference_loss</td><td>0.47188</td></tr><tr><td>test_acc</td><td>0.91797</td></tr><tr><td>test_f1</td><td>0.9145</td></tr><tr><td>test_loss</td><td>0.56277</td></tr><tr><td>test_precision</td><td>0.92188</td></tr><tr><td>test_recall</td><td>0.91358</td></tr><tr><td>train_acc</td><td>0.89656</td></tr><tr><td>train_f1</td><td>0.89813</td></tr><tr><td>train_loss</td><td>0.54405</td></tr><tr><td>train_precision</td><td>0.89983</td></tr><tr><td>train_recall</td><td>0.89825</td></tr><tr><td>train_time</td><td>944.23892</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">classifier_grad_true_lowerLR</strong> at: <a href='https://wandb.ai/andberg9-self/vanilla_AST/runs/gnvg4xlp' target=\"_blank\">https://wandb.ai/andberg9-self/vanilla_AST/runs/gnvg4xlp</a><br/> View project at: <a href='https://wandb.ai/andberg9-self/vanilla_AST' target=\"_blank\">https://wandb.ai/andberg9-self/vanilla_AST</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240917_150807-gnvg4xlp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference_loop(model=model,\n",
    "               device=device,\n",
    "               loss_fn=loss_fn,\n",
    "               inference_loader= inference_dataloader_custom)\n",
    "\n",
    "\n",
    "\n",
    "if not multiple_runs and wandb_init:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving model to: saved_models\\AST_classifier_true.pt\n"
     ]
    }
   ],
   "source": [
    "if SAVE_MODEL:\n",
    "    save_model(model=model,\n",
    "            target_dir=\"saved_models\",\n",
    "            model_name=\"AST_classifier_true.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
