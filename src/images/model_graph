digraph {
	graph [size="33.15,33.15"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	138714713293936 [label="
 ()" fillcolor=darkolivegreen1]
	138714715975504 [label="MeanBackward0
----------------------
self_sym_numel:      9
self_sym_sizes: (1, 9)"]
	138714715974976 -> 138714715975504
	138714715974976 -> 138714713293744 [dir=none]
	138714713293744 [label="mat1
 (1, 256)" fillcolor=orange]
	138714715974976 -> 138714713294320 [dir=none]
	138714713294320 [label="mat2
 (256, 9)" fillcolor=orange]
	138714715974976 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 256)
mat1_sym_strides:       (256, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :       (256, 9)
mat2_sym_strides:       (1, 256)"]
	138714715979008 -> 138714715974976
	138714715979008 [label=ToCopyBackward0]
	138714715979200 -> 138714715979008
	138714713092560 [label="fc2.bias
 (9)" fillcolor=lightblue]
	138714713092560 -> 138714715979200
	138714715979200 [label=AccumulateGrad]
	138714715974544 -> 138714715974976
	138714715974544 -> 138714713294992 [dir=none]
	138714713294992 [label="result1
 (1, 256)" fillcolor=orange]
	138714715974544 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	138714715979296 -> 138714715974544
	138714715979296 -> 138714713295376 [dir=none]
	138714713295376 [label="result
 (1, 256)" fillcolor=orange]
	138714715979296 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	138714715977472 -> 138714715979296
	138714715977472 -> 138714713293648 [dir=none]
	138714713293648 [label="mat1
 (1, 19456)" fillcolor=orange]
	138714715977472 -> 138714713295760 [dir=none]
	138714713295760 [label="mat2
 (19456, 256)" fillcolor=orange]
	138714715977472 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :     (1, 19456)
mat1_sym_strides:     (19456, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :   (19456, 256)
mat2_sym_strides:     (1, 19456)"]
	138714715977328 -> 138714715977472
	138714715977328 [label=ToCopyBackward0]
	138714715976512 -> 138714715977328
	138714713092464 [label="fc1.bias
 (256)" fillcolor=lightblue]
	138714713092464 -> 138714715976512
	138714715976512 [label=AccumulateGrad]
	138714715977424 -> 138714715977472
	138714715977424 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 64, 16, 19)"]
	138714715976464 -> 138714715977424
	138714715976464 -> 138714713293360 [dir=none]
	138714713293360 [label="input
 (1, 64, 16, 19)" fillcolor=orange]
	138714715976464 -> 138714713296624 [dir=none]
	138714713296624 [label="result1
 (64)" fillcolor=orange]
	138714715976464 -> 138714713296912 [dir=none]
	138714713296912 [label="result2
 (64)" fillcolor=orange]
	138714715976464 -> 138714713297104 [dir=none]
	138714713297104 [label="result3
 (0)" fillcolor=orange]
	138714715976464 -> 138714713093136 [dir=none]
	138714713093136 [label="running_mean
 (64)" fillcolor=orange]
	138714715976464 -> 138714713091696 [dir=none]
	138714713091696 [label="running_var
 (64)" fillcolor=orange]
	138714715976464 -> 138714713091984 [dir=none]
	138714713091984 [label="weight
 (64)" fillcolor=orange]
	138714715976464 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	138714715976320 -> 138714715976464
	138714715976320 -> 138714713297200 [dir=none]
	138714713297200 [label="result1
 (1, 64, 16, 19)" fillcolor=orange]
	138714715976320 -> 138714713293456 [dir=none]
	138714713293456 [label="self
 (1, 64, 32, 39)" fillcolor=orange]
	138714715976320 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:         (2, 2)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :         (2, 2)"]
	138714715976032 -> 138714715976320
	138714715976032 -> 138714713297968 [dir=none]
	138714713297968 [label="result
 (1, 64, 32, 39)" fillcolor=orange]
	138714715976032 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	138714715975888 -> 138714715976032
	138714715975888 -> 138714713293264 [dir=none]
	138714713293264 [label="input
 (1, 32, 32, 39)" fillcolor=orange]
	138714715975888 -> 138714713298256 [dir=none]
	138714713298256 [label="weight
 (64, 32, 3, 3)" fillcolor=orange]
	138714715975888 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	138714715975456 -> 138714715975888
	138714715975456 -> 138714713096112 [dir=none]
	138714713096112 [label="input
 (1, 32, 32, 39)" fillcolor=orange]
	138714715975456 -> 138714713298736 [dir=none]
	138714713298736 [label="result1
 (32)" fillcolor=orange]
	138714715975456 -> 138714713299024 [dir=none]
	138714713299024 [label="result2
 (32)" fillcolor=orange]
	138714715975456 -> 138714713299216 [dir=none]
	138714713299216 [label="result3
 (0)" fillcolor=orange]
	138714715975456 -> 138714714468624 [dir=none]
	138714714468624 [label="running_mean
 (32)" fillcolor=orange]
	138714715975456 -> 138714713091120 [dir=none]
	138714713091120 [label="running_var
 (32)" fillcolor=orange]
	138714715975456 -> 138714713091312 [dir=none]
	138714713091312 [label="weight
 (32)" fillcolor=orange]
	138714715975456 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	138714715975264 -> 138714715975456
	138714715975264 -> 138714713299312 [dir=none]
	138714713299312 [label="result1
 (1, 32, 32, 39)" fillcolor=orange]
	138714715975264 -> 138714713293168 [dir=none]
	138714713293168 [label="self
 (1, 32, 64, 78)" fillcolor=orange]
	138714715975264 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:         (2, 2)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :         (2, 2)"]
	138714715974928 -> 138714715975264
	138714715974928 -> 138714713300080 [dir=none]
	138714713300080 [label="result
 (1, 32, 64, 78)" fillcolor=orange]
	138714715974928 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	138714715974784 -> 138714715974928
	138714715974784 -> 138714713293072 [dir=none]
	138714713293072 [label="input
 (1, 16, 64, 78)" fillcolor=orange]
	138714715974784 -> 138714713300368 [dir=none]
	138714713300368 [label="weight
 (32, 16, 3, 3)" fillcolor=orange]
	138714715974784 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	138714715974592 -> 138714715974784
	138714715974592 -> 138714713292880 [dir=none]
	138714713292880 [label="input
 (1, 16, 64, 78)" fillcolor=orange]
	138714715974592 -> 138714713300848 [dir=none]
	138714713300848 [label="result1
 (16)" fillcolor=orange]
	138714715974592 -> 138714713301136 [dir=none]
	138714713301136 [label="result2
 (16)" fillcolor=orange]
	138714715974592 -> 138714713301328 [dir=none]
	138714713301328 [label="result3
 (0)" fillcolor=orange]
	138714715974592 -> 138714713092944 [dir=none]
	138714713092944 [label="running_mean
 (16)" fillcolor=orange]
	138714715974592 -> 138714714466896 [dir=none]
	138714714466896 [label="running_var
 (16)" fillcolor=orange]
	138714715974592 -> 138714714466800 [dir=none]
	138714714466800 [label="weight
 (16)" fillcolor=orange]
	138714715974592 [label="CudnnBatchNormBackward0
----------------------------
epsilon     :          1e-05
input       : [saved tensor]
result1     : [saved tensor]
result2     : [saved tensor]
result3     : [saved tensor]
running_mean: [saved tensor]
running_var : [saved tensor]
training    :           True
weight      : [saved tensor]"]
	138714713309392 -> 138714715974592
	138714713309392 -> 138714713301424 [dir=none]
	138714713301424 [label="result1
 (1, 16, 64, 78)" fillcolor=orange]
	138714713309392 -> 138714713292976 [dir=none]
	138714713292976 [label="self
 (1, 16, 128, 157)" fillcolor=orange]
	138714713309392 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:         (2, 2)
padding    :         (0, 0)
result1    : [saved tensor]
self       : [saved tensor]
stride     :         (2, 2)"]
	138714713309584 -> 138714713309392
	138714713309584 -> 138714713302192 [dir=none]
	138714713302192 [label="result
 (1, 16, 128, 157)" fillcolor=orange]
	138714713309584 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	138714713309728 -> 138714713309584
	138714713309728 -> 138714713302576 [dir=none]
	138714713302576 [label="input
 (1, 1, 128, 157)" fillcolor=orange]
	138714713309728 -> 138714713302672 [dir=none]
	138714713302672 [label="weight
 (16, 1, 3, 3)" fillcolor=orange]
	138714713309728 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 1)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	138714713309920 -> 138714713309728
	138714713309920 [label=ToCopyBackward0]
	138714713310112 -> 138714713309920
	138714713310112 [label="UnsqueezeBackward0
------------------
dim: 1"]
	138714713310208 -> 138714713310112
	138714713310208 [label="UnsqueezeBackward0
------------------
dim: 0"]
	138714713310256 -> 138714713310208
	138714713310256 [label=ToCopyBackward0]
	138714713310448 -> 138714713310256
	138714713095824 [label="
 (128, 157)" fillcolor=lightblue]
	138714713095824 -> 138714713310448
	138714713310448 [label=AccumulateGrad]
	138714713309776 -> 138714713309728
	138714713309776 [label=ToCopyBackward0]
	138714713310016 -> 138714713309776
	138714714466224 [label="conv1.0.weight
 (16, 1, 3, 3)" fillcolor=lightblue]
	138714714466224 -> 138714713310016
	138714713310016 [label=AccumulateGrad]
	138714713309488 -> 138714713309728
	138714713309488 [label=ToCopyBackward0]
	138714713310400 -> 138714713309488
	138714714468816 [label="conv1.0.bias
 (16)" fillcolor=lightblue]
	138714714468816 -> 138714713310400
	138714713310400 [label=AccumulateGrad]
	138714713309344 -> 138714715974592
	138714714466800 [label="conv1.3.weight
 (16)" fillcolor=lightblue]
	138714714466800 -> 138714713309344
	138714713309344 [label=AccumulateGrad]
	138714713309296 -> 138714715974592
	138714714468912 [label="conv1.3.bias
 (16)" fillcolor=lightblue]
	138714714468912 -> 138714713309296
	138714713309296 [label=AccumulateGrad]
	138714715974640 -> 138714715974784
	138714715974640 [label=ToCopyBackward0]
	138714713309632 -> 138714715974640
	138714713091024 [label="conv2.0.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	138714713091024 -> 138714713309632
	138714713309632 [label=AccumulateGrad]
	138714715975168 -> 138714715974784
	138714715975168 [label=ToCopyBackward0]
	138714713309968 -> 138714715975168
	138714713091216 [label="conv2.0.bias
 (32)" fillcolor=lightblue]
	138714713091216 -> 138714713309968
	138714713309968 [label=AccumulateGrad]
	138714715975312 -> 138714715975456
	138714713091312 [label="conv2.3.weight
 (32)" fillcolor=lightblue]
	138714713091312 -> 138714715975312
	138714715975312 [label=AccumulateGrad]
	138714715975360 -> 138714715975456
	138714713091408 [label="conv2.3.bias
 (32)" fillcolor=lightblue]
	138714713091408 -> 138714715975360
	138714715975360 [label=AccumulateGrad]
	138714715975840 -> 138714715975888
	138714715975840 [label=ToCopyBackward0]
	138714715974880 -> 138714715975840
	138714713091792 [label="conv3.0.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	138714713091792 -> 138714715974880
	138714715974880 [label=AccumulateGrad]
	138714715976176 -> 138714715975888
	138714715976176 [label=ToCopyBackward0]
	138714715975216 -> 138714715976176
	138714713091888 [label="conv3.0.bias
 (64)" fillcolor=lightblue]
	138714713091888 -> 138714715975216
	138714715975216 [label=AccumulateGrad]
	138714715977184 -> 138714715976464
	138714713091984 [label="conv3.3.weight
 (64)" fillcolor=lightblue]
	138714713091984 -> 138714715977184
	138714715977184 [label=AccumulateGrad]
	138714715976560 -> 138714715976464
	138714713092080 [label="conv3.3.bias
 (64)" fillcolor=lightblue]
	138714713092080 -> 138714715976560
	138714715976560 [label=AccumulateGrad]
	138714715979152 -> 138714715977472
	138714715979152 [label=TBackward0]
	138714715976080 -> 138714715979152
	138714715976080 [label=ToCopyBackward0]
	138714715975408 -> 138714715976080
	138714713092368 [label="fc1.weight
 (256, 19456)" fillcolor=lightblue]
	138714713092368 -> 138714715975408
	138714715975408 [label=AccumulateGrad]
	138714715974496 -> 138714715974976
	138714715974496 [label=TBackward0]
	138714715977280 -> 138714715974496
	138714715977280 [label=ToCopyBackward0]
	138714715975936 -> 138714715977280
	138714713092656 [label="fc2.weight
 (9, 256)" fillcolor=lightblue]
	138714713092656 -> 138714715975936
	138714715975936 [label=AccumulateGrad]
	138714715975504 -> 138714713293936
}
