services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - DOCKER_BUILDKIT=1
    image: ast_container
    container_name: ast_script_container
    environment:
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONUNBUFFERED=1
      - DEBIAN_FRONTEND=noninteractive
      - WANDB_API_KEY=${WANDB_API_KEY}
      - BOT_TOKEN=${BOT_TOKEN}
      - CHAT_ID=${CHAT_ID}
      - NVIDIA_VISIBLE_DEVICES=all  # Changed from CUDA_VISIBLE_DEVICES
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility  # Add driver capabilities
      - PL_TRAINER_STRATEGY=auto  # Add environment variable for PyTorch Lightning strategy

    volumes:
      - ./src:/app/src  # Only mount the src directory as read-only
      - ./src/datasets/.:/app/src/datasets
      - ./src/model_cache:/app/src/model_cache  # Mount the model_cache directory
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, utility]  # Added more capabilities
    # command: python3 script.py
    command: python3 main.py
    # command: python3 sweeps.py
    # command: /bin/bash # interactive shell new
    stdin_open: true
    tty: true
    working_dir: /app/src  # Change this to /app/src
    # user: "appuser"
    user: root
    networks:
      - app_network
    shm_size: 8gb  # Add shared memory size for DDP communication
    # runtime: nvidia  # Add explicit nvidia runtime

volumes:
  datasets:
    driver: local
  src:  # Add this named volume
  model_cache:  # Add this named volume

networks:
  app_network:
    driver: bridge

# Add a .env file with the following content to set default compose options
# COMPOSE_COMMAND_OPTIONS=--remove-orphans